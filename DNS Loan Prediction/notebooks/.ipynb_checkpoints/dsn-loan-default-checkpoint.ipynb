{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\nwarnings.filterwarnings('ignore')\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Read datasets\ntraindemographics = pd.read_csv('../input/traindemographics.csv', parse_dates=['birthdate'])\ntestdemographics = pd.read_csv('../input/testdemographics.csv', parse_dates=['birthdate'])\ntrainperf = pd.read_csv('../input/trainperf.csv', parse_dates=['approveddate','creationdate'])\ntestperf = pd.read_csv('../input/testperf.csv',parse_dates=['approveddate','creationdate'])\ntrainprevloans= pd.read_csv('../input/trainprevloans/trainprevloans.csv', parse_dates=['approveddate','creationdate','closeddate','firstduedate','firstrepaiddate'])\ntestprevloans = pd.read_csv('../input/testprevloans/testprevloans.csv', parse_dates=['approveddate','creationdate','closeddate','firstduedate','firstrepaiddate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ef6507d29a16e8fdc448032aada08f5a930a5c6"},"cell_type":"code","source":"#Print the size and shape of the data\nprint('traindemographics shape:', traindemographics.shape)\nprint('testdemographics shape:', testdemographics.shape)\nprint('trainperf shape:', trainperf.shape)\nprint('testperf shape:', testperf.shape)\nprint('trainprevloans shape:', trainprevloans.shape)\nprint('testprevloans shape:', testprevloans.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c69642b7522bf8d4996579825aa51a1e19f322c8"},"cell_type":"markdown","source":"Let's preview all features"},{"metadata":{"trusted":true,"_uuid":"f16bc98e1aad5db64a2f4423a37e5b2d33c10ee7"},"cell_type":"code","source":"traindemographics.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6f878535756cd8a5b46a8395be63556b061b94c"},"cell_type":"code","source":"testdemographics.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcf4073701559be366ee0b8ab8601de56420ac1a"},"cell_type":"code","source":"trainperf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43ff871be9eb012e2ee728283cdc9a195c31e799"},"cell_type":"code","source":"testperf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7757610d00ffa3d391b29ffbbc6813bc40e4ed39"},"cell_type":"code","source":"trainprevloans.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5249bb526b9d4aa19d982f987aa121dff4ef61f4"},"cell_type":"code","source":"testprevloans.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29b156fa923163f5ad8d7fbc90dcebf11fb1d89b"},"cell_type":"markdown","source":"Let's explore our data and create a base line model.\nWe'll start with the main train data **trainperf** do some feature engineering and make a prediction."},{"metadata":{"trusted":true,"_uuid":"4bc76a5f07c1e71fd4835b5e499f2a7f0ff679e0"},"cell_type":"code","source":"trainperf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a13625bb44229b54e352a20321c3776781c40b3"},"cell_type":"code","source":"def process_perf_data(df, dataset='train', to_categorical=True):\n    #drop systemloanid and loannumber\n    proc_df = df.copy()\n    #get the interest to be paid on the loan\n    proc_df['interest'] = proc_df['totaldue'] - proc_df['loanamount']\n    \n    # Convert days to year by dividing with 365)\n    proc_df['termyears'] = proc_df['termdays'] / 365\n    #Drop original days column\n    proc_df.drop(['termdays'], axis=1, inplace=True)\n    #get the rate of interest R = I/P*T \n    proc_df['rateofinterest'] = proc_df['interest'] / (proc_df['loanamount'] * proc_df['termyears'])\n    \n    #Convert referredby to a boolean column of yes and no\n    #First, get the null values index\n    indx_null = proc_df[proc_df['referredby'].isnull()].index\n    # Get the index of not null values\n    indx_not_null = proc_df[~proc_df['referredby'].isnull()].index\n    proc_df['referredbysomeone'] = proc_df['referredby']\n    proc_df['referredbysomeone'][indx_null] = False\n    proc_df['referredbysomeone'][indx_not_null] = True\n    #Drop referredby column\n    proc_df.drop(['referredby'], axis=1, inplace=True)\n    \n    #Get some information from approveddate and creationdate columns\n    #First create dictionary keys for dayofweek and month of year\n    dic_dayofweek = {0:'mon',1:'tue',2:'wed',3:'thur',4:'fri',5:'sat',6:'sun'}\n    dic_month = {1:'jan', 2:'feb',3:'mar',4:'apr',5:'may',6:'jun',7:'jul',8:'aug', 9:'sept',10:'oct',11:'nov',12:'dec'}\n    \n#     #Convert to categorical if processing trainperf\n#     if to_categorical:\n#         proc_df['approveddayofweek'] = proc_df['approveddate'].dt.dayofweek.map(dic_dayofweek)\n#         proc_df['approvedmonth'] = proc_df['approveddate'].dt.month.map(dic_month)\n#         proc_df['creationdayofweek'] = proc_df['creationdate'].dt.dayofweek.map(dic_dayofweek)\n#         proc_df['creationmonth'] = proc_df['creationdate'].dt.month.map(dic_month)\n#     else:\n#         #Working on previous loans where the numbers are important\n#         proc_df['total_loan_duration'] = proc_df['closeddate'] - proc_df['creationdate']\n#         proc_df['total_loan_duration'] = proc_df['total_loan_duration'].dt.seconds\n        \n#         proc_df['approveddayofweek'] = proc_df['approveddate'].dt.dayofweek\n#         proc_df['approvedmonth'] = proc_df['approveddate'].dt.month\n#         proc_df['creationdayofweek'] = proc_df['creationdate'].dt.dayofweek\n#         proc_df['creationmonth'] = proc_df['creationdate'].dt.month\n        \n#     proc_df['is_month_start_approved'] = proc_df['approveddate'].dt.is_month_start\n#     proc_df['is_month_end_approved'] = proc_df['approveddate'].dt.is_month_end\n#     proc_df['is_month_start_creation'] = proc_df['creationdate'].dt.is_month_start\n#     proc_df['is_month_end_creation'] = proc_df['creationdate'].dt.is_month_end\n    \n#     #Get the approval time elapsed in seconds\n#     proc_df['loan_approval_time_elapsed'] = proc_df['approveddate'] - proc_df['creationdate']\n#     proc_df['loan_approval_time_elapsed'] = proc_df['loan_approval_time_elapsed'].dt.seconds\n    \n    #Drop the date columns\n    proc_df.drop(['creationdate','approveddate'], axis=1, inplace=True)\n    \n    if dataset == 'train':\n        #Convert the target column to boolean values: 0 ===> Good, 1 ==>Bad\n        dict_target = {\"Good\": 1, \"Bad\": 0}\n        target = proc_df['good_bad_flag'].map(dict_target)\n        proc_df.drop(['good_bad_flag'], axis=1, inplace=True)\n        return (proc_df, target)\n    else:\n        return proc_df\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3798fd7f876da111336c8153deeb7dfc14f86341"},"cell_type":"code","source":"new_trainperf,target = process_perf_data(trainperf)\nnew_testperf = process_perf_data(testperf, dataset='test')\nnew_trainperf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9afa263a6db7a6347f298a23f9aa4c2bd534e8e7"},"cell_type":"code","source":"new_testperf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3f8f0997fca1dfa4dfbbf9011bca9f212298cac"},"cell_type":"code","source":"def get_percent_of_missing(df):\n    missing = (df.isnull().sum() / len(df)) * 100\n    missing = missing.drop(missing[missing == 0].index).sort_values(ascending=False)\n    missing_data = pd.DataFrame({'Missing Ratio' :missing})\n    print(missing_data)\n    return\n\ndef align_dataframes(train,test):\n    # Align the training and testing data. This is done because of the onehot encoding which \n    # introduces extra columns into the train data. We keep only columns present in both dataframes\n    train, test = train.align(test, join = 'inner', axis = 1)\n    return (train,test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c59f46f53984acdc5db6789e399ca9efa80ce489"},"cell_type":"code","source":"get_percent_of_missing(new_trainperf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c08968103c6f0962b71fd77aefc1daa3219d72cf"},"cell_type":"code","source":"get_percent_of_missing(new_testperf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f40dac2614410839ec8b0b31eade784f59c825bf"},"cell_type":"code","source":"# #Correlation map to see how features are correlated with SalePrice\n# corrmat = new_trainperf.corr()\n# plt.subplots(figsize=(10,8))\n# sns.heatmap(corrmat, vmax=0.9, square=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"872b34d5765bce98dc72845627d9ea17f8b2ae24"},"cell_type":"code","source":"#Drop the ID columns\nnew_trainperf.drop(['systemloanid','loannumber'], axis=1, inplace=True)\nnew_testperf.drop(['systemloanid','loannumber'], axis=1, inplace=True)\n\n# X = pd.get_dummies(new_trainperf.drop(['customerid'], axis=1, inplace=False))\n# print('New shape of trainperf is', X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8aadc669852dcbcc9f4915e5472400c0b50af98"},"cell_type":"code","source":"#Let's look the target variable\ntarget.astype(int).plot.hist()\n#We can see that we have an inbalance class problem","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6590c94395af1047c06e76c69617a6e2cd539eaa"},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error,make_scorer\nimport xgboost as xgb\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7f9622c6db0c4456ae8008b5f6098bb4add75c3"},"cell_type":"code","source":"def custom_scorer_func(ground_truth,predictions):\n    err = np.sum(ground_truth != predictions)\n    err = err/len(ground_truth)\n    return err\n\ncustom_scorer = make_scorer(custom_scorer_func, greater_is_better=False)\n    \ndef cross_val(model, data, label):\n    sf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42).get_n_splits(data)\n    percent_loss = cross_val_score(model, data, label, scoring= custom_scorer, cv = sf)\n    return(-1 * percent_loss) #Remove negative sign\n\n\ndef plot_importance(df):\n    #Sort values by index\n    df = df.sort_values('importance', ascending = False).reset_index()\n    #Normalize between 0 and 1\n    df['importance'] = df['importance'] / df['importance'].sum()\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:40]))), \n            df['importance'].head(40), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:40]))))\n    ax.set_yticklabels(df['feature'].head(40))\n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"455c60e5c21bb416401909fbcffbb621e6a36b7b"},"cell_type":"code","source":" model_lgb = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.001, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4817e6aff0e65b7e1a73af76484912e47a29bea4"},"cell_type":"code","source":"# print(cross_val(model_lgb,X,target))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"406b5a7e3b13e325bab965344301395f91999e5f"},"cell_type":"code","source":"#  model_lgb.fit(X, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"560674e158ef97ee20d126a3819dced54f45c163"},"cell_type":"code","source":"# feature_importances = pd.DataFrame({'feature': list(X.columns), 'importance': model_lgb.feature_importances_ })\n# plot_importance(feature_importances)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dca5b4eee1729afbed25dab54253642edded716"},"cell_type":"markdown","source":"Now, Let's look at the other data, starting with the previous loan data set"},{"metadata":{"trusted":true,"_uuid":"90a003169f7476909b0db8b188459e6831abbd1f"},"cell_type":"code","source":"trainprevloans.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba6667590b1ebb55fc52a37c7a88c06587a33959"},"cell_type":"markdown","source":"**Most of the columns are similar to the train performance data. So we can use the same function as earlier**"},{"metadata":{"trusted":true,"_uuid":"2eb81631c725faa4cbc41aaf5144366260b3711a"},"cell_type":"code","source":"new_trainprev = process_perf_data(trainprevloans, dataset='none', to_categorical=False)\nnew_testprev = process_perf_data(testprevloans, dataset='none', to_categorical=False)\nnew_trainprev.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8517054907c2411453fa070f32c125ad313135a"},"cell_type":"code","source":"new_testprev.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c147466839cc006f2da4ba1d61e2a463aca6a74e"},"cell_type":"markdown","source":"**Let's create a function to clean the remaining columns**"},{"metadata":{"trusted":true,"_uuid":"2faed0fac73ad76991dd0f56ba85a2daca2eec9a"},"cell_type":"code","source":"def process_prevloans(df):\n    df = df.copy()\n    df['diff_due_and_repay'] = df['firstrepaiddate'] - df['firstduedate']\n    df['diff_due_and_repay'] = df['diff_due_and_repay'].dt.seconds\n    \n    df['diff_closed_and_due'] = df['closeddate'] - df['firstduedate']\n    df['diff_closed_and_due'] = df['diff_closed_and_due'].dt.seconds\n    \n    df['due_dayofweek'] = df['firstduedate'].dt.dayofweek\n    df['due_month'] = df['firstduedate'].dt.month\n    \n    df['repaid_dayofweek'] = df['firstrepaiddate'].dt.dayofweek\n    df['repaid_month'] = df['firstrepaiddate'].dt.month\n    \n    df['is_month_start_repaid'] = df['firstrepaiddate'].dt.is_month_start\n    df['is_month_end_repaid'] = df['firstrepaiddate'].dt.is_month_end\n    \n    df['is_month_start_duedate'] = df['firstduedate'].dt.is_month_start\n    df['is_month_end_duedate'] = df['firstduedate'].dt.is_month_end\n    \n    #DRop old date columns\n    df.drop(['closeddate','firstduedate','firstrepaiddate'],axis=1,inplace=True)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b861b6b249eb2a0aec19393ae8e28f1262992dae"},"cell_type":"code","source":"new_trainprev = process_prevloans(new_trainprev)\nnew_testprev = process_prevloans(new_testprev)\nprint('Shape of new_trainprev:', new_trainprev.shape)\nprint('Shape of new_testprev:', new_testprev.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26913d60d668d85ed0d64bfc202eddd600a81d8a"},"cell_type":"code","source":"# def get_loan_counts(df):\n#     # Groupby the customer id and count the number of previous loans, and rename the column\n#     counts_df = df.groupby('customerid', as_index=False)['systemloanid'].count().rename(columns = {'systemloanid': 'previous_loan_counts'})\n#     return counts_df\n\n# def get_refer_counts(df):\n#     # Groupby the customer id and count the number of previous loans, and rename the column\n#     refer_counts = df.groupby('customerid', as_index=False)['referredby'].count().rename(columns = {'referredby': 'num_referrals'})\n#     return refer_counts\n\ndef merge_2_df(df1,df2):\n    # Join two dataframe\n    df = df1.merge(df2, how='left',on='customerid')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ffdc310474aac0d95b46dfff94bdd35272da45b"},"cell_type":"code","source":"def calculate_agg(df):\n    # Group by the customer id, calculate aggregation statistics\n    df_agg = df.drop(columns = ['systemloanid']).groupby('customerid', as_index = False).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n    # List to hold column names\n    columns = ['customerid']\n    # Iterate through the variables names\n    for var in df_agg.columns.levels[0]:\n        # Skip the id name\n        if var != 'customerid':      \n            # Iterate through the calculated stat names\n            for stat in df_agg.columns.levels[1][:-1]:\n                # Make a new column name for the variable and stat\n                columns.append('prevloan_%s_%s' % (var, stat))\n    \n    # Assign the list of columns names as the dataframe column names\n    df_agg.columns = columns\n    return df_agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ada431ee4a6185a17906027c952048ca7f42c1a"},"cell_type":"code","source":"train_2_merge = calculate_agg(new_trainprev)\ntest_2_merge = calculate_agg(new_testprev)\n\nnew_X = merge_2_df(new_trainperf,train_2_merge)\ntest = merge_2_df(new_testperf,test_2_merge)\n\nmerged_data = new_X.copy()\nnew_X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c437b818d4ad918b4a606d5e15f64bcfe6b77a66"},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0cadb88fb6bf0cf6fc84967fd894f3a11248151"},"cell_type":"code","source":"# new_X.drop(['customerid'], axis=1,inplace=True)\n# new_X = pd.get_dummies(new_X)\n# print(\"New data shape is\", new_X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f8a0d17768d8e525c1733308dd82a32002bba22"},"cell_type":"code","source":"# print(cross_val(model_lgb,new_X,target))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94b9e6b879d465bcd05641b3ca7d1ea513ba1ea7"},"cell_type":"code","source":"# model_lgb.fit(new_X,target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a11500e91bf7bd3b6a4e5005cdd85af35bea6c1"},"cell_type":"code","source":"# feature_importances = pd.DataFrame({'feature': list(new_X.columns), 'importance': model_lgb.feature_importances_ })\n# plot_importance(feature_importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cb94e657abdaf569c7b08a8d4c2f8ba329c8222"},"cell_type":"markdown","source":"**Let's add the final dataset Train demographics**\n"},{"metadata":{"trusted":true,"_uuid":"fac71cbea5a1a10f7be79f6e068d21a7093d144b"},"cell_type":"code","source":"traindemographics.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79099b89ea0da6d7744eb3a5d8a58ce7fba7a708"},"cell_type":"code","source":"get_percent_of_missing(traindemographics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be2d52ca1a47ee370598b009103e68b942265db1"},"cell_type":"code","source":"# traindemographics['employment_status_clients'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15df23c9f175781aa472509408c592b0fdfe6f5e"},"cell_type":"code","source":"def process_demographics(df):\n    proc_df = df.copy()\n    #Let's extract info from the birthdate\n    proc_df['customers_age'] = 2018 - proc_df['birthdate'].dt.year\n    #We'll drop the bank_branch_clients and level_of_education_clients because they contain too many missing values\n    proc_df.drop(['level_of_education_clients','bank_branch_clients','birthdate'], axis=1, inplace=True)\n    return proc_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"017da5fb1156cf53cee632bb83b2665b42129ebd"},"cell_type":"code","source":"new_traindemo = process_demographics(traindemographics)\nnew_test = process_demographics(testdemographics)\n\nnew_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"015eb5d30a6512e2a3f575e41a729e2902dec48c"},"cell_type":"code","source":"merged_data = merged_data.drop_duplicates('customerid')\ntest = test.drop_duplicates('customerid')\nnew_traindemo = new_traindemo.drop_duplicates('customerid')\nnew_test = new_test.drop_duplicates('customerid')\n\nfinal_data = merge_2_df(merged_data,new_traindemo)\nfinal_test = merge_2_df(test,new_test)\n\nfinal_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"913f00f951e4fd13d393e0f9b0a7bba2580c2128"},"cell_type":"code","source":"final_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d3d249e3173973f3a5a4c63644bd7baaeb5eb04"},"cell_type":"code","source":"final_data = pd.get_dummies(final_data)\nfinal_test = pd.get_dummies(final_test)\n\nfinal_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbfffb83a1251c48840f8a195df4ec9bbce708cc"},"cell_type":"code","source":"final_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a491a9dceeccb2f435c03a370e59259c14e988f3"},"cell_type":"code","source":"#Align dataset\nfinal_train, final_test = final_data.align(final_test, join = 'inner', axis = 1)\nprint(\"Shape of final train is:\", final_train.shape)\nprint(\"Shape of final test is:\", final_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd3772656cf3a03a98f7794299c928dcc0fd7796"},"cell_type":"code","source":"print(cross_val(model_lgb,final_train,target))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75c879371468becd65c0202c50eb62a7c2c90015"},"cell_type":"code","source":"model_lgb.fit(final_train,target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"affd4d8dc39b97f3fabad7ca4f18e4e8a8307d71"},"cell_type":"code","source":"feature_importances = pd.DataFrame({'feature': list(final_train.columns), 'importance': model_lgb.feature_importances_ })\nplot_importance(feature_importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10794ab6ba82013b873f74609039919e10b328af"},"cell_type":"code","source":"pred = model_lgb.predict(final_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec679d6fad95477f7daf4683aa5035e7f5b67ecb"},"cell_type":"code","source":"samplesub = pd.read_csv('../input/SampleSubmission.csv')\nsamplesub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29055dd0f51a4f32a0120277323dc3ed0615e6ad"},"cell_type":"code","source":"testperf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0b64cf4ed1b5e430c733d937ff093693ca2dcec"},"cell_type":"code","source":"samplesub['Good_Bad_flag'] = pred\nsamplesub['customerid'] = testperf['customerid']\n\nsamplesub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e78e92d3be63f71ca9df1c97febb299ef4a611c9"},"cell_type":"code","source":"samplesub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c14acf9de941ee4ee63cbcc825a08e4a0318a402"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}